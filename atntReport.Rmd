---
title: "**Analysis to Predict Closing Stock Prices from Company AT&T**"
author: 
- Victor Yung
- Benson Ou-yang
- Ivan Cao

abstract:
  In our project, our goal was to develop a suitable prediction model by 
  assessing the closing stock prices of the company AT&T using linear 
  regression analysis. To determine the regressors we used stepwise 
  analysis, which discovered various sets of variables that resulted 
  in having a significant relationship to the closing price of AT&T stocks 
  which includes volume, capital surplus, gross margin, and liabilities. 
  By fitting each regressor into the model, we were able to produce a model 
  that explained roughly 66.4% of variability of predicting the closing price. 
  Furthermore, we used visual graphs such as normal q-q plots and Residual 
  plots to help identify any underlying issues with patterns or outliers we 
  came across in our model. The overall analysis of our model helped us subset 
  and distinguish the effectiveness of each regressor we found to our response 
  variable.

output: 
  bookdown::pdf_document2:
    extra_dependencies: "subfig"
    fig_caption: yes
    includes: 
      in_header: my_header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.height=3.5, fig.path='Figs/', fig.pos = "H",
                      echo=FALSE, warning=FALSE, message=FALSE)
```

```{r package-options, include=FALSE}
knitr::opts_knit$set(progress = TRUE, verbose = TRUE)
```

# Introduction

The telecommunication space is an inevitably growing industry dependent on the 
advancement of technology. Therefore, there will be stock investment 
opportunities for consumers to take part in as we notice that the amount of 
potential this industry presents, however it is not risk free.  In our analysis,
we designate our efforts toward one of the most well-known telecommunication 
companies around the world, AT&T. Our question of interest is to predict the 
closing stock price of the years 2012 to 2016 for AT&T.
 In order to tackle this problem, we first collected and constructed the data 
 set with categories related to closing stock prices and tested to see if 
 there is a strong significance to those specific years. We then performed a 
 stepwise procedure to ensure the categories we chose had a strong significance 
 to the closing price. Therefore, we introduced those variables as the regressor
 we will use for our final model for prediction. The variables include volume 
 measuring the number of shares traded during a specific time, capital surplus
 which is the excess remaining after common stock sold,  gross margin as the 
 percentage of the difference between revenue and cost of goods sold divided 
 by revenue, and lastly liabilities being how much a company owes.  The model 
 will be further dissected through visual plots that will explain the different 
 patterns and possible outliers that may affect the results of our final model.


```{r, echo = FALSE,message= FALSE}
library(lubridate)
library(tidyverse)
library(faraway)
library(caret)
library(reshape2)
library(car)
```


```{r, echo = FALSE}
# securities is the dataset containing sector information 
securities = read.csv("securities.csv")

# fund contains yearly fundamental information about each company 
fund = read.csv("fundamentals.csv")

# psa contains adjusted prices of daily stock prices
psa = read.csv("prices-split-adjusted.csv")

# adjusted prices for company AT&T

tprices = psa[psa$symbol == "T",]

# reset indexes
row.names(tprices) = NULL
```


```{r,echo = FALSE}
# change date to datetime object
tprices$date = ymd(tprices$date)

# placeholder for column names in for loop
fundminus = select(fund,-c("X","Ticker.Symbol","Period.Ending","For.Year"))


# separate fund data corresponding to tprices only
ex = fund[fund$Ticker.Symbol == "T",]

# adding last years yearly info into present year
tprices$year = year(tprices$date)-1
tprices = tprices[tprices$year %in% unique(ex$For.Year),]

# for loops to add all columns of fund into tprices

for(j in unique(ex$For.Year)){
  for(k in names(fundminus)){
    tprices[tprices$year == j, k] = ex[ex$For.Year == j, k]
  }
}


tprices = select(tprices, c("close","volume","date", "year",
                            "Capital.Surplus","Gross.Margin","Liabilities"))

tprices$year = year(tprices$date)

```



```{r,echo = FALSE}
# adding a data point at the end 
added_obs = data.frame("2017-01-01", 30, 98323500, 9.1038e+10, 57, 1798000000)
names(added_obs) = c("date", "close", "volume", "Capital.Surplus", 
                     "Gross.Margin", "Liabilities")
added_obs$date = ymd(added_obs$date)
added_obs$year = year(added_obs$date)

row.names(tprices) = NULL
# the last row is the added observation
tprices = rbind(tprices, added_obs)


```

# Data Description

We are using three datasets: \verb|securities.csv|, \verb|fundamentals.csv|, and 
\verb|prices-split-adjusted.csv|.

\verb|Securities.csv| contains information on the stock companies such as the 
company's name and ticker symbol, the type of sector they are in, location of 
headquarters and others. 

\verb|Fundamentals.csv| contains information of yearly reports of fundamental 
information of each company such as \verb|total| \verb|revenue|, \verb|accounts| 
\verb|payable|, \verb|liabilities| and many more.

\verb|Prices-split-adjusted.csv| contains information of the stocks adjusted 
prices after splitting. The columns included are the \verb|date|, \verb|ticker| 
\verb|symbol|, \verb|close|, \verb|open|, \verb|low|, \verb|high|,and 
\verb|volume|.


```{r, echo = FALSE}
mdl = lm(close ~ volume + Capital.Surplus + Gross.Margin + Liabilities ,
         tprices)
```


For our linear regression model, we have selected to predict close prices of 
AT&T’s stock. Our regressor variables are volume from the 
\verb|prices-split-adjusted.csv| and \verb|capital surplus|, \verb|gross margin|
, and \verb|liabilities| from the \verb|fundamentals.csv|. See Equation 
\@ref(eq:mlr) 

\begin{equation}
  close = \beta_0 + \beta_1(volume) + \beta_2(Capital Surplus) + 
  \beta_3(Gross Margin) + \beta_4(Liabilities) + \epsilon (\#eq:mlr)
\end{equation}


Since the data from \verb|fundamentals.csv| is yearly, we applied the previous 
year data into the next year since the yearly reports are at the end of the year 
so we use that information for the next year. For example, if the \verb|total| 
\verb|revenue| for 2013 is $1,000,000 so we made a column for total revenue and 
made every row that is in 2014 to be $1,000,000. We ran a nested for loop to 
apply this for all years and columns. \verb|Tprices| is the DataFrame with the 
columns of interest for our linear model. See Table \@ref(tab:names-tab).

```{r names-tab}

knitr::kable(names(tprices), format = "markdown", 
             caption = "Names of DataFrame Tprices", col.names = "Columns")
```


\verb|Close| corresponds to the price of the stock when the market closes.

\verb|Volume| is the number of trades that occurred that day.

\verb|Date| is the date of the trading day.

\verb|Year| is the year of the trading day.

\verb|Capital.surplus| or share premium, most commonly refers to the surplus 
resulting after common stock is sold for more than its par value.

\verb|Gross.margin| is a company's net sales revenue minus its cost of goods 
sold. The higher the gross margin, the more capital a company retains on each 
dollar of sales, which it can then use to pay other costs or satisfy debt 
obligations.

\verb|Liabilities| are the debts and obligations of a company.


```{r c-plot,fig.cap="Daily Market Close Price of AT&T"}
ggplot(data = tprices , aes(x = date, y = close)) + geom_line() + 
    ggtitle("Market Close Price of AT&T Over the Years") + xlab("Time") + 
    ylab("Close Price($)") + 
    geom_point(aes(x = tprices[date == "2013-01-02", "date"], 
                   y = tprices[date == "2013-01-02", "close"]), color = "red") + 
    geom_point(aes(x = tprices[date == "2014-01-02", "date"], 
                   y = tprices[date == "2014-01-02", "close"]), color = "red") + 
    geom_point(aes(x = tprices[date == "2015-01-02", "date"], 
                   y = tprices[date == "2015-01-02", "close"]), color = "red") + 
    geom_point(aes(x = tprices[date == "2016-01-04", "date"], 
                   y = tprices[date == "2016-01-04", "close"]), color = "red")
```


Figure \@ref(fig:c-plot) represents the market closing price of the stock for 
AT&T over the years. The red points on the line is just the indicator of the 
beginning of the year. Over the years, the closing price is around $35 starting 
the year in 2013 and 2014. Around spring of 2013, the stock shot up to about $39
which is the highest closing price until 2016. The stock drops to about $32 in 
the beginning of the year and before 2015. In 2016, the stock was starting to 
rise and in mid 2016, the stock got a new record of about $43.

```{r, echo = FALSE}
summary(tprices$close)
```

```{r,echo = FALSE, results='hide'}
head(sort(tprices$close, decreasing = FALSE))
```

In the five number summary of the close prices, the minimum is $30 but that is 
the one data point that we have added. So without that data point, the lowest is
\$31.80. The max close price is \$43.47. The mean close price is \$35.77 since
the mean wouldn't change that much from one data point that is \$30.


```{r v-plot, fig.cap="Daily Volume Traded of ATNT"}
ggplot(data = tprices, 
       aes(x = year, y = volume, group = year, fill = year)) + geom_boxplot() +
        ggtitle("Volume of Trades of AT&T Throughout the Years")

```

Figure \@ref(fig:v-plot) shows the volume traded of each year. The mean volume 
traded is about the same for every year except for 2015. The dots of the boxplot
represent the outliers of each year. There are days where the stock is traded 
more often that usual such as when the stock is low, more people are buying and 
when the stock is high, more are selling. The max volume traded was in 2016, we 
can assume a lot of people were selling when AT&T stock was at its highest in 
this data.

```{r,echo = FALSE}
print("Summary of Volume in 2013")
summary(tprices[tprices$year == 2013,"volume"])
print("Summary of Volume in 2014")
summary(tprices[tprices$year == 2014,"volume"])
print("Summary of Volume in 2015")
summary(tprices[tprices$year == 2015,"volume"])
print("Summary of Volume in 2016")
summary(tprices[tprices$year == 2016,"volume"])
```

```{r cs-plot,fig.cap="Capital Surplus of ATNT 2013-2016"}
# Capital surplus, or share premium, most commonly refers to the surplus 
# resulting after common stock is sold for more than its par value.

ggplot(data = tprices, aes(x = year, y = Capital.Surplus)) + geom_point() +
  labs(x = "Year", y = "Capital Surplus", 
         title = "Capital Surplus of AT&T Over the Years")
```

Figure \@ref(fig:cs-plot) shows the capital surplus of each year. It was rising 
up until 2016 where it dropped by a lot.

```{r l-plot, fig.cap="Liabilities of ATNT 2013-2016"}
# The debts and obligations of a company or an individual. Current liabilities 
# are debts due and payable within one year. Long-term liabilities are those 
# payable after one year.

ggplot(data = tprices, aes(x = year, y = Liabilities)) + geom_point() +
  labs(x = "Year", 
         title = "Liabilities of AT&T Over the Years")
```

Figure \@ref(fig:l-plot) shows the liabilities of each year. In 2013 and 2015, 
AT&T had the highest liabilities. In 2014, they had the lowest. In 2016, it was 
between the highest and lowest liabilities.

```{r gm-plot, fig.cap="Gross Margin of ATNT 2013-2016"}
# Gross margin is a company's net sales revenue minus its cost of goods sold 
# (COGS). ... The higher the gross margin, the more capital a company retains 
# on each dollar of sales, which it can then use to pay other costs or satisfy 
# debt obligations.

ggplot(data = tprices, aes(x = year, y = Gross.Margin)) + geom_point() +
  labs(x = "Year", y = "Gross Margin", 
         title = "Gross Margin of AT&T Over the Years")
```

Figure \@ref(fig:gm-plot) shows the gross margin of each year. From 2013 to 
2014, it rose up to the highest of 60. In 2015 it fell down to 55 and 2016 
dropped to 54.

```{r pp-plot, fig.cap = "Pairs Plot of Columns of Tprices",fig.height=3.25}
pairs(tprices)
```

Figure \@ref(fig:pp-plot) shows the columns plotted against each other. Since 
the columns from the fundamentals.csv is yearly data, when plotted against other
columns, they are shown as separate lines due to the values being the same daily 
for the year.

```{r h-map,fig.cap = "Correlation Between Variables", fig.width = 8, fig.height = 5}
# adapted from http://www.sthda.com/english/wiki/ggplot2-quick-correlation-
# matrix-heatmap-r-software-and-data-visualization

# gathered the columns we needed and made data frame into a matrix

tmat = as.matrix(select(tprices,-c("date","year")))

# get the correlation between the columns
tcor = round(cor(tmat),2)

# function to just get the upper triangle of values for heatmap
get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
}

# reorder the correlation matrix based on its correlation

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}

tcor = reorder_cormat(tcor)

upper_tri = get_upper_tri(tcor)

# melt for plottable data

melted = melt(upper_tri,na.rm = TRUE)

# produces heat map

heatmap = ggplot(data = melted, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed() + geom_text(aes(Var2, Var1, label = value), color = "black", 
                           size = 4) +
theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.grid.major = element_blank(),
  panel.border = element_blank(),
  panel.background = element_blank(),
  axis.ticks = element_blank(),
  legend.justification = c(1, 0),
  legend.position = c(0.5, 0.7),
  legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5)) + 
  ggtitle("Heatmap of Correlation Between Regressors")

print(heatmap)

```

Figure \@ref(fig:h-map) shows the correlation between each column.

# Methods

## Datasets

To begin, we chose to combine two of the four datasets offered, 
“Price-split-adjusted” (PSA) and “Fundamentals” (Fund). The PSA dataset accounts
for all stocks traded in the NYSE daily from 2010-2016 and the Fund dataset 
accounts for the 10-K filing from 2012-2016, an annual comprehensive report 
required by the U.S. Securities and Exchanges Commission (SEC). Since the dates
in the datasets varies from daily in PSA and annually in Fund, we attach the 
previous year’s filing of the 10-K report to help predict the next year over.

## Subsetting Dataset

Since our datasets covers different years, we kept only the years that overlap 
in both datasets, 2013-2016. Lastly, we removed all companies other than our 
company of focus, AT&T. With 76 columns in \verb|fundamentals.csv| that were 
added into \verb|tprices| DataFrame, we ran into issues with fitting the model 
with all the columns. The error that came up was due to singularities with the
columns. The singularities are due to the fact that many of the variables are
dependent of each other.The model would fit up to four variables and the other 
coefficients would be NA. Due to this, we handpicked the variables 
\verb|Capital.Surplus|, \verb|Liabilities|, and \verb|Gross.Margins|. We
subsetted the DataFrame to just include \verb|close|, \verb|volume|, \verb|date|,
\verb|year|, \verb|year|, \verb|Capital.Surplus|, \verb|Gross.Margin|, and
\verb|Liabilities|. Table \@ref(tab:t-tab) presents the subsetted data. 

```{r t-tab}
knitr::kable(tprices[1:5,1:7], format="markdown", 
             caption = "First five rows of tprices DataFrame")
```

## Model Adequacy

Checking model adequacy is an important step to measure the accuracy of the model. 
The Residuals vs. Fitted values in Figure \@ref(fig:ma-plot)(a) shows whether the 
residuals have a relationship with each other. Ideally, the points would be 
randomly scattered about zero with no patterns. In our plot, the spaces in 
between the four groups represent the four different years ranging from 
2013-2016. When observing the residuals vs fitted, it seems as though the points
are scattered randomly about zero, starting with a negative relationship into a 
more stable relationship as the plot moves from left to right. We also see that 
the scatter on the far right Is more spread out which could point out to 
potential problems down the line.


The normal Q-Q plot in Figure \@ref(fig:ma-plot)(b) shows whether the errors are 
normally distributed. If the errors were normally distributed, points would be 
rested on the line with minimum gaps between the line and the points. Our plot 
shows that the ends of the plot has a noticeable gap between the line and the 
points and there are more points near the middle of the points that lies almost 
directly on the line. Our Q-Q plot seems to be light tailed but nonetheless the 
plot is about normal. Our plot points out point 761 and 763 as potential 
outliers.


```{r ma-plot, fig.cap='Model Adequacy Plots', fig.subcap=c('Residuals vs Fitted Plot','Normal QQ Plot','Scale-Location Plot','Residuals vs Leverage'),fig.height = 3.5, fig.ncol = 2,out.width = "50%"}

# Model Adequacy 
plot(mdl, which = 1)
plot(mdl, which = 2)
plot(mdl, which = 3)
plot(mdl, which = 5)
```


The scale-location plot in Figure \@ref(fig:ma-plot)(c) shows whether the 
residuals are spread equally among the predictors and whether the assumption 
of constant variance is met. Ideally, the plot show has points scattered equally
horizontally. In our plot, we see that there are four groups of points, the 
first three groups (from left to right) seem to follow constant variance but the
points in the rightmost group have a wider spread compared to the previous 
three. The residuals in the plot do not meet constant variance assumption. 
Our plot points out point 761 and 763 as potential outliers.


The residuals vs leverage plot in Figure \@ref(fig:ma-plot)(d) points out the 
influential observations within our dataset. The dotted line represents the 
cook’s distance and any points that fall outside of the dotted red line 
signifies a highly influential point to the regression results. If we were to 
exclude these observations, our regression will change and improve. In our case,
no points fall outside of the cook’s distance, this may be due to the large 
number of observations included in the data. The plot did highlight point 65, 
396, and 1009 (our added point) as potential outliers.


```{r av-plot, fig.cap = "Added Variable Plot of Model"}
avPlots(mdl)
```

The Added Variable Plots in Figure \@ref(fig:av-plot) shows the linear
relationship between the regressors and response variable. For the regressors
\verb|Capital.Surplus|, \verb|Gross.Margin|, and \verb|Liabilities|, there are
chunks of points because this is due to these columns having one value daily of
that year thus affecting these plots.

We also investigated the leverage and influential points. A leverage point is
when $h_{ii}$ > $2p/n$, where \verb|p| is the number of predictors and \verb|n|
is the number of rows of the dataset. $h_{ii}$ comes from the diagonal elements 
of the hat matrix(\@ref(eq:hat)):

\begin{equation}
  \boldsymbol{H} = \boldsymbol{X} (\boldsymbol{X}'\boldsymbol{X})^{-1}  
  \boldsymbol{X} (\#eq:hat)
\end{equation}

The high leverage points are sorted here.

```{r lev-tab}
# leverage point if hii > 2p/n

influ = influence(mdl) # measures leverage points
# head(sort(influ$hat, decreasing = TRUE))

# decreasing leverage points
sort(influ$hat[influ$hat>(2*4)/nrow(tprices)],decreasing = TRUE)

```



```{r}
# influential point if Di > 1

cooks = cooks.distance(mdl) # measures influential points
# head(sort(cooks,decreasing = TRUE))
# which(cooks > 1) # no influential points
```


```{r}
# remove leverage points and assess
rowindex = names(influ$hat[influ$hat>(2*4)/nrow(tprices)])
df = tprices

# for loop to make id column to reference row
for(i in 1:nrow(tprices)){
  df[i,"id"] = i
}

newdata = df[!(df$id %in% c(rowindex)),]
newdata = select(newdata,-"id")

no_lev_mdl = lm(close~volume + Capital.Surplus + Gross.Margin + Liabilities ,
         newdata)
```

```{r nolev-plot, fig.cap='Model Adequacy Plots', fig.subcap=c('Residuals vs Fitted Plot','Normal QQ Plot','Scale-Location Plot','Residuals vs Leverage'),fig.height = 3.5, fig.ncol = 2,out.width = "50%"}

# Model Adequacy 
plot(no_lev_mdl, which = 1)
plot(no_lev_mdl, which = 2)
plot(no_lev_mdl, which = 3)
plot(no_lev_mdl, which = 5)
```

By removing the leverage points, the model adequacy plots in Figure 
\@ref(fig:nolev-plot) seem almost identical. There seems to be less clustering 
of points without the leverage points and for the residuals vs leverage plot
in Figure \@ref(fig:nolev-plot)(d) looks better as you can see the data points
more clearer on the left side.

```{r}
summary(no_lev_mdl)
```

By removing the leverage points, the $R^2$ improves from 0.6638 to 0.6832.

Here we checked the Variance Inflation Factor(\verb|VIF|),

```{r}
vif(mdl)
```

The \verb|VIF| for \verb|Capital.Surplus|, \verb|Gross.Margin|, and 
\verb|Liabilities| were pretty high but they are less than 10 so 
multicollinearity is not an issue.

## Model Selection

By doing stepwise selection with our model, we found that we already have the 
best model and don't need to remove any variables. See Equation \@ref(eq:mlr) 
above.

```{r}
step(no_lev_mdl)
```

## Model Validation

After assessing the model adequacy, we go on to validate our model to see if it 
can function properly and successfully for the intended user. To do this, 
we sampled 80% of the AT&T data to form the training dataset, leaving 20% to 
be the testing set. The model and results will be shown in the following section.

```{r}
set.seed(75392)

df = newdata

nsamp=ceiling(0.8*length(df$close))
training_samps=sample(c(1:length(df$close)),
                      nsamp)
training_samps=sort(training_samps)
train_data  <- df[training_samps, ]
test_data <-   df[-training_samps, ]


train.lm <- lm(close ~ volume + Capital.Surplus + Gross.Margin + Liabilities, 
               data = df)

preds <- predict(train.lm,test_data)
```


# Result

This is the summary of fitting a model on the training data.

```{r}
summary(train.lm)
```


```{r pred-p, fig.cap = "Predicted Values Plotted with Actual Values With y=x Line"}
plot(test_data$close,preds, xlab = "Actual Close Price", 
     ylab = "Predicted Close Price", main = "Actual vs Predicted Close Price")
abline(c(0,1))
```


Figure \@ref(fig:pred-p) shows the relationship between predicted close price 
(y-axis) versus actual close price (x-axis). We notice that the fitted line 
provides a positive slope across the plot indicating that there is a positive 
linear relationship between the predicted and actual close price. In addition, 
the points are roughly evenly scattered around the fitted line which is a sign 
of constant variance in the graph however, because there is variation around the
fitted line it means that our plot does not perfectly predict the closing 
prices. We observed the two points on the top left of the graph which is an 
indication of potential outliers however, a rule of thumb if we get rid of those
two points it will not make a significant difference in the overall pattern of 
the plot therefore they can be left alone.

```{r info-tab}
R.sq = R2(preds, test_data$close)
RMSPE = RMSE(preds, test_data$close)
MAPE = MAE(preds, test_data$close)
NSE=RMSPE/sd(test_data$close)
# print(c(R.sq,RMSPE,MAPE,NSE))

info = data.frame("R2" = R.sq, "RMSPE" = RMSPE, "MAPE" = MAPE, "NSE" = NSE)

knitr::kable(info, format = "markdown", 
             caption = "Table containing $R^{2}_{Prediction}$, Root Mean Square 
             Prediction Error, Mean Absolute Prediction Error, 
             Normalized Standard Error")
```  

In Table \@ref(tab:info-tab), we generated the $R^{2}_{Prediction}$, Root Mean
Square Prediction Error, Mean Absolute Prediction Error and Normalized Standard
Error. The $R^{2}_{Prediction}$ is like doing a regression with the independent
variable as the predicted values and the dependent variable is the testing 
values. It tells us how well our regression model makes predictions. Our model
doesn't predict poorly but also doesn't predict that well. 
  
  
The Root Mean Square Prediction Error (Equation \@ref(eq:RMSPE)) is the standard 
deviation of the residuals. It is a measure of how far the data points deviate 
from the regression line.
  
  
\begin{equation}
  \boldsymbol{RMSPE} = \sqrt{\frac{1}{\boldsymbol{n}}\sum_{i=1}^{n}
  (\boldsymbol{y_i} - \boldsymbol{\hat{y_i}})^2}
  (\#eq:RMSPE)
\end{equation}
  
  
The Mean Absolute Prediction Value measures the average magnitude of the data 
points that deviate from the fitted line. See Equation \@ref(eq:MAPE).
  
  
\begin{equation}
  \boldsymbol{MAPE} = \frac{1}{\boldsymbol{n}}\sum_{i=1}^{n}
  |\boldsymbol{y_i} - \boldsymbol{\hat{y_i}}|
  (\#eq:MAPE)
\end{equation}
  
  
The Normalized Standard Error(Equation \@ref(eq:NSE)) is the normalized Root Mean
Square Prediction Error. Normalizing it tells us if the Root Mean Square 
Prediction Error value is a low value or not. It tells us how much variability 
we have explained. For example, a value of 1 says the model explains none of the 
variability. 
  
  
\begin{equation}
  \boldsymbol{NSE} = \frac{\boldsymbol{RMSPE}}{\boldsymbol{\sigma_{test}}}
  (\#eq:NSE)
\end{equation}
  
  

```{r test-plot, fig.cap="Time Series Plot of Actual Close Price and Predicted Close Price"}
plot(test_data$date, test_data$close, type = 'l', 
     main = "AT&T Close Price Over Time", ylab = "Close Price" ,xlab = "Time")
lines(test_data$date, preds, col = "red")
legend(x = "bottomright",legend = c("Actual","Predicted"),pch = "___", 
       col = c("black","red"))
```

Figure \@ref(fig:test-plot) shows the overall pattern of close prices of AT&T 
stocks over years 2013 to 2017. This plot is a great tool that allows us to 
compare visually the difference between the predicted closing prices labelled 
in red versus the actual closing prices labelled in black. We see that our 
predicted prices are generally more stable than the actual prices meaning that 
there are some variations between what we predicted versus what the actual price
of the closing stock is at a certain point in time. From the results we see our
prediction is not 100% accurate but it does show that it is not completely off 
as it provides similar patterns to the actual closing price. Furthermore, we 
also notice that large shifts in patterns particularly from 2016 to 2017 where 
closing price has been the highest which is an indication that the value of the 
company has grown.

# Conclusion



# Appendix

## URL: 
[Repository containing all files](https://github.com/bensonouyang/NYStocks)  

## Data Files: 
[fundamentals.csv](https://github.com/bensonouyang/NYStocks/blob/main/fundamentals.csv)  
[prices-split-adjusted.csv](https://github.com/bensonouyang/NYStocks/blob/main/prices-split-adjusted.csv)  
[securities.csv](https://github.com/bensonouyang/NYStocks/blob/main/securities.csv)  

## Raw Code:
[rawcode.Rmd](https://github.com/bensonouyang/NYStocks/blob/main/rawcode.Rmd)  

## Report:
[atntReport.Rmd](https://github.com/bensonouyang/NYStocks/blob/main/at%26t.Rmd)  
[atntReport.pdf](https://github.com/bensonouyang/NYStocks/blob/main/at-t.pdf)  

## Text Files:
[README.md](https://github.com/bensonouyang/NYStocks/blob/main/README.md)  
[my_header.tex](https://github.com/bensonouyang/NYStocks/blob/main/my_header.tex)  

## Required Libraries:

\verb|lubridate|  
\verb|tidyverse|  
\verb|faraway|  
\verb|caret|  
\verb|reshape2|  
\verb|car|  
\verb|knitr|
\verb|bookdown|

